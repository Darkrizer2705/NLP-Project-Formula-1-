{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8f73d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c13b669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>race context the 2025 spanish grand prix, offi...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the pandemic-era austrian gp marked f1's retur...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>race context the 2023 saudi arabian grand prix...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>race context the 2025 miami grand prix, offici...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>race context the 2025 austrian grand prix, off...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  race context the 2025 spanish grand prix, offi...      5\n",
       "1  the pandemic-era austrian gp marked f1's retur...      8\n",
       "2  race context the 2023 saudi arabian grand prix...      7\n",
       "3  race context the 2025 miami grand prix, offici...      5\n",
       "4  race context the 2025 austrian grand prix, off...      2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"data/processed/train.csv\")\n",
    "test_df = pd.read_csv(\"data/processed/test.csv\")\n",
    "\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "871e7eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(train_df['label'].unique())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac4d159d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 892,425 || all params: 67,852,818 || trainable%: 1.3152\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_lin\", \"v_lin\"],   # DistilBERT attention layers\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfc76401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5822317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b8913eef264be686179f34659a27e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd6a77df47d45b181f3b0da5dbbc4ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert pandas DataFrames to Hugging Face Datasets and tokenize\n",
    "train_data = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "test_data = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "# Tokenize datasets\n",
    "train_data_tokenized = train_data.map(tokenize, batched=True)\n",
    "test_data_tokenized = test_data.map(tokenize, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40de98c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "# Build training args as a dict, then filter to supported kwargs\n",
    "training_kwargs = {\n",
    "    \"output_dir\": \"distilbert_lora_finetuned\",\n",
    "    \"per_device_train_batch_size\": 8,\n",
    "    \"per_device_eval_batch_size\": 8,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"num_train_epochs\": 4,\n",
    "    \"logging_steps\": 10,\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"skip_memory_metrics\": True,\n",
    "    \"push_to_hub\": False,\n",
    "}\n",
    "\n",
    "# Filter out unsupported kwargs for compatibility with different transformers versions\n",
    "from inspect import signature\n",
    "try:\n",
    "    sig = signature(TrainingArguments.__init__)\n",
    "    supported = set(sig.parameters.keys()) - {\"self\", \"args\", \"kwargs\"}\n",
    "    filtered_kwargs = {k: v for k, v in training_kwargs.items() if k in supported}\n",
    "except Exception:\n",
    "    # Fallback: if inspection fails, pass the full dict and let TrainingArguments raise a clear error\n",
    "    filtered_kwargs = training_kwargs\n",
    "\n",
    "training_args = TrainingArguments(**filtered_kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bc70077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shriv\\OneDrive\\Desktop\\NLP project\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:32, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.188600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.116400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shriv\\OneDrive\\Desktop\\NLP project\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\shriv\\OneDrive\\Desktop\\NLP project\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\shriv\\OneDrive\\Desktop\\NLP project\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=2.1524888038635255, metrics={'train_runtime': 34.2001, 'train_samples_per_second': 4.211, 'train_steps_per_second': 0.585, 'total_flos': 9736233467904.0, 'train_loss': 2.1524888038635255, 'epoch': 4.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data_tokenized,\n",
    "    eval_dataset=test_data_tokenized,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69e0db5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/distilbert_lora\\\\tokenizer_config.json',\n",
       " 'models/distilbert_lora\\\\special_tokens_map.json',\n",
       " 'models/distilbert_lora\\\\vocab.txt',\n",
       " 'models/distilbert_lora\\\\added_tokens.json',\n",
       " 'models/distilbert_lora\\\\tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"models/distilbert_lora\")\n",
    "tokenizer.save_pretrained(\"models/distilbert_lora\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbce69be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(text):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0991642",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = joblib.load(\"models/label_encoder.pkl\")\n",
    "\n",
    "def predict_driver(text):\n",
    "    label = predict_label(text)\n",
    "    return le.inverse_transform([label])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0948e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Winner: Lewis Hamilton\n"
     ]
    }
   ],
   "source": [
    "sample = \"\"\"\n",
    "McLaren delivered by far the strongest performance of the entire weekend, with Lando Norris \n",
    "completely dominating every practice session. Norris set the fastest laps in FP1, FP2, and \n",
    "FP3 by a large margin and showed unmatched consistency during long-run simulations. His \n",
    "average race-pace was over four tenths quicker than any other driver on track.\n",
    "\n",
    "Oscar Piastri could not match Norris's speed, and the McLaren engineers confirmed that \n",
    "Norris's setup was the best they have produced all season. Ferrari struggled severely \n",
    "with tyre degradation, and both Leclerc and Sainz were more than half a second slower in \n",
    "their high-fuel stints.\n",
    "\n",
    "Mercedes were completely off the pace the entire weekend. George Russell reported major \n",
    "balance issues, while Lewis Hamilton lost over six tenths per lap in race trim and never \n",
    "came close to Norris's pace at any point. Hamilton was nowhere near the front of the field \n",
    "in any session.\n",
    "\n",
    "All analysts, team strategists, and tyre engineers unanimously agree that **Lando Norris \n",
    "is the clear favourite** and most likely winner of this Grand Prix based on every metric.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"Predicted Winner:\", predict_driver(sample))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
