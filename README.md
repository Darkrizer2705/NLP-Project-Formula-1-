ğŸï¸ **F1 Race Winner Prediction using NLP + DistilBERT + LoRA**

# ğŸï¸ Formula 1 Race Winner Prediction using NLP & DistilBERT (LoRA Finetuning)

This project predicts **the winner of a Formula 1 Grand Prix** using **text-based race summaries**, practice reports, and weekend analysis.
It applies modern **NLP techniques**, **data balancing**, and **LoRA-based transformer finetuning** to classify which driver is most likely to win.

---

# ğŸ“Œ **Project Motivation**

F1 race summaries contain rich contextual information:

* Driver performance
* Team upgrades
* Practice/qualifying pace
* Weather
* Tyre degradation
* Car balance/handling
* Engineering analysis

A neural network can learn these patterns and predict the likely race winner.

---

# ğŸš€ **Tech Stack**

| Component  | Technology                                 |
| ---------- | ------------------------------------------ |
| Language   | Python                                     |
| NLP Model  | DistilBERT (LoRA Finetuning)               |
| Finetuning | PEFT (LoRA)                                |
| Dataset    | Custom F1 text dataset (2020â€“2025)         |
| UI         | Streamlit Web App                          |
| Training   | CPU-friendly (no GPU required)             |
| Libraries  | Transformers, PEFT, Datasets, Scikit-learn |

---

# ğŸ—‚ï¸ **Folder Structure**

```
NLP PROJECT/
â”‚
â”œâ”€â”€ Data/
â”‚   â”œâ”€â”€ f1_dataset.csv
â”‚   â”œâ”€â”€ balanced_f1_dataset.csv
â”‚   â””â”€â”€ (raw data files)
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_preprocessing.ipynb
â”‚   â”œâ”€â”€ 01.0_balancing_dataset.ipynb
â”‚   â”œâ”€â”€ 02_modeltraining.ipynb
â”‚   â”œâ”€â”€ 03_prediction.ipynb
â”‚   â”œâ”€â”€ 04_finetuning.ipynb
â”‚   â”œâ”€â”€ 05_app.ipynb
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ distilbert_lora/
â”‚       â”‚     â”œâ”€â”€ config.json
â”‚       â”‚     â”œâ”€â”€ pytorch_model.bin
â”‚       â”‚     â”œâ”€â”€ adapter_config.json
â”‚       â”‚     â””â”€â”€ other tokenizer/model files
â”‚       â”œâ”€â”€ bert_model.pt
â”‚       â””â”€â”€ label_encoder.pkl
â”‚
â”œâ”€â”€ app.py                      # Streamlit App
â”œâ”€â”€ README.md                   # You are here
â””â”€â”€ requirements.txt
```

---

# ğŸ“š **Dataset Preparation**

### 1ï¸âƒ£ **Raw Data**

Collected F1 race weekend summaries (FP1/FP2/FP3, Qualifying, Race reports) from 2020â€“2024.

### 2ï¸âƒ£ **Cleaning**

* Lowercasing
* Removing special characters
* Removing extra whitespace
* Standardizing driver/team names

### 3ï¸âƒ£ **Label Encoding**

Winners were encoded using:

```python
LabelEncoder()
```

### 4ï¸âƒ£ **Major Issue: Class Imbalance**

Most races were won by:

* Max Verstappen
* Lewis Hamilton

Other drivers (Norris, Russell, Leclerc, Piastri) were heavily under-represented.

### 5ï¸âƒ£ **Balancing**

A custom balancing script:

* Oversampled rare winners
* Undersampled majority winners
* Resulted in equal samples per driver
* Fixed strong prediction bias

Final dataset stored as:

```
balanced_f1_dataset.csv
```

---

# ğŸ§  **Model Architecture**

## ğŸ¯ 1. Baseline: TF-IDF + Dense NN

(Built initially for testing)

## ğŸ¯ 2. Transformer Models Tried

### âŒ Qwen2-0.5B + QLoRA

* Too slow on CPU
* BitsAndBytes 4-bit layers failed backward pass
* Not suitable for Windows-CPU environment

### âœ… Final Model: **DistilBERT + LoRA Finetuning**

Chosen because:

* Lightweight
* Very fast on CPU
* Finetunes quickly (~10â€“20 seconds)
* Excellent performance
* Works perfectly with LoRA adapters (PEFT)

---

# ğŸ”§ **Training Pipeline (DistilBERT + LoRA)**

### âœ” Tokenization (256 max length)

### âœ” LoRA adapters added to DistilBERT attention layers

### âœ” Optimizer: AdamW

### âœ” Batch size: 8

### âœ” Epochs: 4

### âœ” Loss function: Cross Entropy

### âœ” PEFT saves only LoRA weights (very small)

Training notebook:

```
04_finetuning.ipynb
```

---

# ğŸ§ª **Prediction Pipeline**

Prediction steps:

1. Load tokenizer + LoRA finetuned DistilBERT
2. Encode input text
3. Pass through classifier
4. Convert label â†’ driver using stored label encoder

Notebook for predictions:

```
03_prediction.ipynb
```

---

# ğŸŒ **Streamlit App**

File:

```
app.py
```

Features:

* Text box for entering race weekend summary
* Predict button
* Outputs winning driver
* Shows clear UI messages

Run using:

```
streamlit run app.py
```

---

# ğŸ“ˆ **Results**

### After dataset balancing + LoRA finetuning:

* Predictions became **more fair**
* Less biased toward Verstappen/Hamilton
* Model correctly identifies strong patterns for:

  * Lando Norris
  * Charles Leclerc
  * George Russell
  * Oscar Piastri
  * Sergio PÃ©rez

### Accuracy improved significantly

(based on balanced dataset split)

---

# ğŸ§­ **Key Learnings**

* Raw transformer finetuning can be slow on CPU
* QLoRA needs GPU to function properly
* DistilBERT is very powerful for small datasets
* Dataset balancing is **critical** in motorsport NLP
* LoRA drastically reduces training time
* Streamlit brings the model to life with a clean UI

---

# ğŸš€ **How to Run the Project**

### 1. Clone Repo

```
git clone https://github.com/<your-username>/f1-nlp-winner-prediction.git
cd f1-nlp-winner-prediction
```

### 2. Install Requirements

```
pip install -r requirements.txt
```

### 3. Run Preprocessing (optional)

```
jupyter notebook notebooks/01_preprocessing.ipynb
```

### 4. Train Model

```
jupyter notebook notebooks/04_finetuning.ipynb
```

### 5. Run Streamlit App

```
streamlit run app.py
```



# âœ¨ **Author**

**Shrivatsh and Pranav**
Mahindra University
NLP Project â€” Formula 1 Winner Prediction

---


